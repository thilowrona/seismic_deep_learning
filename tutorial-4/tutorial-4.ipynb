{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "tutorial_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUTQmbtAMW8-"
      },
      "source": [
        "# How to map faults in 3-D\n",
        "\n",
        "This tutorial describes how to map tectonic faults in a 3-D seismic volume using a 3-D convolutional neural network, but only 2-D labels using a masked loss function. Using 2-D labels is really useful, because 3-D labelling is really time-consuming (you would need to label a large number of closely-spaced 2-D lines). So this is a nice trick! Huge thanks to Valentin Tschannen and co-authors for showing us how to do this! Check out their excellent paper: [https://doi.org/10.1190/geo2019-0569.1](https://doi.org/10.1190/geo2019-0569.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0wQrs-EMW8_"
      },
      "source": [
        "## Setup ##\n",
        "1. First, you will need sign in to your Google account. If you're not signed in, you can sign in [here](https://myaccount.google.com/?utm_source=sign_in_no_continue)\n",
        "2. Next, head on to the [Colab Welcome Page](https://colab.research.google.com/notebooks/welcome.ipynb#recent=true).\n",
        "3. There, select Github in the top tab and search for https://github.com/thilowrona/seismic_deep_learning/blob/master/tutorial_4/tutorial_4.ipynb\n",
        "4. Clicking opens this notebook. This is a Jupyter notebook; an awesome combination of code and documentation allowing us work on, describe and share our projects.\n",
        "5. Before running anything in here, you need to tell Colab that would like to use a GPU (important to train our models quickly). This is done by clicking on the ‘Runtime’ tab, selecting ‘Change runtime type’, changing hardware accelerator to ´GPU´ and clicking ´save´. Excellent! Colab is now set up!\n",
        "6. When you run the first cell, you will face a pop-up saying ‘Warning: This notebook was not authored by Google’; you should click on ‘Run Anyway’ to get rid of the warning.\n",
        "7. Next we want to save our notebook. If you click on ‘File’ and then ‘Save’, you will see a pop-up saying ´CANNOT SAVE CHANGES´. Now, click on ‘SAVE A COPY IN DRIVE’. This opens up a new tab with the same file, but this time located in your Drive. If you want to continue working after saving, use the file in the new tab. Your notebook will be saved in a folder called Colab Notebooks in your Google Drive by default.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzYfnds6MzmD"
      },
      "source": [
        "## Pre-processing ##\n",
        "Now that everything is set up, we need to install and load a couple of python packages, we will use:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRoDrPuuZnAy"
      },
      "source": [
        "!pip install googledrivedownloader\n",
        "!pip install segpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3ocfSAiEO6f"
      },
      "source": [
        "from sys import stdout\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "import segpy\n",
        "from segpy.reader import create_reader\n",
        "\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "from google.colab import widgets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Dnmps7Ze0o"
      },
      "source": [
        "and clone our git repository:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzNIKxSCSOW0"
      },
      "source": [
        "!git clone https://github.com/thilowrona/seismic_deep_learning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zPGyM4LZyZP"
      },
      "source": [
        "and download the 3-D seismic volume:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbJEt_0dLMHa"
      },
      "source": [
        "gdd.download_file_from_google_drive(file_id='196UkQqciaAkS9P78lbIPr39Qf_DvlcBN',\n",
        "                                    dest_path='./Seismic_data.sgy',\n",
        "                                    unzip=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KT8hHkqai5F"
      },
      "source": [
        "and load the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WB9rgrpCydw"
      },
      "source": [
        "filename = \"./Seismic_data.sgy\"\n",
        "    \n",
        "with open(filename, 'rb') as segy:\n",
        "    segy_reader = segpy.reader.create_reader(segy)\n",
        "    data = np.zeros((segy_reader.num_inlines(), segy_reader.num_trace_samples(1), segy_reader.num_xlines()))\n",
        "    for inline_num, xline_num in segy_reader.inline_xline_numbers():\n",
        "        trace_index = segy_reader.trace_index((inline_num, xline_num))        \n",
        "        inline_start = segy_reader.inline_numbers()[0]\n",
        "        xline_start  = segy_reader.xline_numbers()[0]        \n",
        "        data[inline_num-inline_start,:,xline_num-xline_start] = segy_reader.trace_samples(trace_index)\n",
        "\n",
        "data = (data-np.min(data))/(np.max(data)-np.min(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHf1sVLQ3dbj"
      },
      "source": [
        "and plot it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB0wde39Y8hV"
      },
      "source": [
        "def load_label(inline):\n",
        "    label = Image.open('./seismic_deep_learning/tutorial-3/mask_inline_' + str(inline) + '.png')\n",
        "    label.load()\n",
        "    label = np.asarray(label)[:,:,3]\n",
        "    label = np.where(label > 0.5, 1, 0)\n",
        "    return label\n",
        "\n",
        "\n",
        "def plot_overlay(image, label):\n",
        "    (tmax, xmax) = np.shape(image)\n",
        "\n",
        "    label_rgb = np.zeros((tmax,xmax,4), 'uint8')\n",
        "    label_rgb[:,:,0] = 255\n",
        "    label_rgb[:,:,1] = 255 - 255*label\n",
        "    label_rgb[:,:,2] = 255 - 255*label\n",
        "    label_rgb[:,:,3] = 255*label   \n",
        "    \n",
        "    img = Image.fromarray(label_rgb, mode='RGBA')\n",
        "    \n",
        "    image_rgb = np.zeros((tmax,xmax,4), 'uint8')    \n",
        "    image_rgb[:,:,0] = 255*image\n",
        "    image_rgb[:,:,1] = 255*image\n",
        "    image_rgb[:,:,2] = 255*image\n",
        "    image_rgb[:,:,3] = 255\n",
        "    \n",
        "    background = Image.fromarray(image_rgb, mode='RGBA')    \n",
        "    background.paste(img, (0, 0), img)\n",
        "\n",
        "    plt.imshow(background)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "\n",
        "\n",
        "tb = widgets.TabBar([str(i) for i in range(100,501,100)])\n",
        "\n",
        "for i in range(5): \n",
        "  with tb.output_to(i):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    label = load_label(100+100*i)\n",
        "    plot_overlay(data[100+100*i,:,:], label)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02vN-DXm88up"
      },
      "source": [
        "Now we can define function (cubify), which generates our training examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n-e55Cp60tK"
      },
      "source": [
        "def cubify(data, inline, batch_size, size, n_classes, threshold):\n",
        " \n",
        "  # position where the 2D slice has been interpreted\n",
        "  x_idx = size//2\n",
        "\n",
        "  data3D  = np.zeros((batch_size,size,size,size,1),dtype=np.float32)\n",
        "  label2D = np.zeros((batch_size,size,size,n_classes),dtype=np.float32)\n",
        "\n",
        "  label = load_label(inline) \n",
        "\n",
        "  (t_max, x_max) = data[0,:,:].shape\n",
        "  x_centers = np.random.randint(size//2, x_max-size//2, batch_size)\n",
        "  t_centers = np.random.randint(size//2, t_max-size//2, batch_size)\n",
        "\n",
        "  n = 0\n",
        "  while n < batch_size: \n",
        "      x = random.randint(size//2, x_max-size//2)\n",
        "      t = random.randint(size//2, t_max-size//2)\n",
        "      if np.count_nonzero(label[t-size//2:t+size//2, x-size//2:x+size//2]) > threshold:\n",
        "          data3D[n,:,:,:,0]  = data[inline-size//2:inline+size//2,t-size//2:t+size//2, x-size//2:x+size//2]\n",
        "          label2D[n,:,:,0]  = label[t-size//2:t+size//2, x-size//2:x+size//2]\n",
        "          n = n + 1\n",
        "\n",
        "\n",
        "  # Initialize 3D labels with zeros\n",
        "  label3D = np.zeros((batch_size,size,size,size,n_classes),dtype=np.float32) # shape is [x,y,z,n_classes]\n",
        "\n",
        "  # Write 2D labels at correct position\n",
        "  label3D[:,x_idx,...] = label2D\n",
        "\n",
        "  # Create 3D binary mask. 1 where we have valid labels, 0 elsewhere\n",
        "  mask = np.zeros((batch_size,size,size,size,1),dtype=np.float32) # shape is [x,y,z]\n",
        "  mask[:,x_idx,:,:] = 1.\n",
        "\n",
        "  return data3D, label3D, mask\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oZ3TNpC9les"
      },
      "source": [
        "Creating a large number of these 3-D examples will quickly fill up our memory, so we will create a data generator to load our data during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48wfI8xQUUo2"
      },
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self,\n",
        "                 data,\n",
        "                 inlines,\n",
        "                 n_exp,\n",
        "                 dim, \n",
        "                 threshold,\n",
        "                 batch_size=32, \n",
        "                 shuffle=True):\n",
        "        \n",
        "        'Initialization'\n",
        "        self.data   = data\n",
        "        self.inlines = inlines\n",
        "        self.n_exp  = n_exp\n",
        "        self.dim    = dim\n",
        "        self.threshold = threshold\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.n = 0\n",
        "        self.max = self.__len__()\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(self.n_exp / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate data\n",
        "        X, Y, mask = self.__data_generation()\n",
        "\n",
        "        return X, Y, mask\n",
        "\n",
        "\n",
        "    def __data_generation(self):\n",
        "        'Generates data containing batch_size samples'\n",
        "        \n",
        "        # Initialization\n",
        "        X    = np.zeros((self.batch_size, *self.dim))\n",
        "        Y    = np.zeros((self.batch_size, *self.dim))\n",
        "        mask = np.zeros((self.batch_size, *self.dim))\n",
        "\n",
        "        self.per = self.batch_size//len(self.inlines)  \n",
        "\n",
        "        # Generate data\n",
        "        n = 0\n",
        "        for inline in self.inlines:\n",
        "            X[self.per*n:self.per*(n+1),...], Y[self.per*n:self.per*(n+1),...], mask[self.per*n:self.per*(n+1),...] = cubify(data, inline, self.per, self.dim[0], 1, self.threshold)            \n",
        "            n = n + 1\n",
        "                        \n",
        "        return X, Y, mask\n",
        "\n",
        "    def __next__(self):\n",
        "      if self.n >= self.max:\n",
        "        self.n = 0\n",
        "      result = self.__getitem__(self.n)\n",
        "      self.n += 1\n",
        "      return result\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRl6fn27HdRL"
      },
      "source": [
        "Now we create a data generator object with our data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XxHxpEAUU-9"
      },
      "source": [
        "# Parameters\n",
        "size       = 32\n",
        "threshold  = 0\n",
        "num_train  = 1000\n",
        "num_val    = 200\n",
        "(tmax, xmax) = (4001, 8605)\n",
        "dim = (size,size,size,1)\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "\n",
        "\n",
        "inlines_train = [100, 200, 400, 500]                  \n",
        "inlines_val   = [300]\n",
        "\n",
        "\n",
        "\n",
        "datagen_train = DataGenerator(data,\n",
        "                              inlines_train,\n",
        "                              num_train,\n",
        "                              dim,\n",
        "                              threshold,\n",
        "                              batch_size=32, \n",
        "                              shuffle=True)\n",
        "\n",
        "datagen_val   = DataGenerator(data,\n",
        "                              inlines_val,\n",
        "                              num_val,\n",
        "                              dim,\n",
        "                              threshold,\n",
        "                              batch_size=32, \n",
        "                              shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrm1-n1QHjF4"
      },
      "source": [
        "Check the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F46AO-liArmP"
      },
      "source": [
        "X_train_batch, Y_train_batch, mask_train_batch = next(datagen_train)\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(2, 10, figsize=(15,3))\n",
        "\n",
        "k = 0\n",
        "for m in range(10):\n",
        "  axs[0,m].imshow(X_train_batch[k,size//2,:,:,0], interpolation='spline16', cmap=plt.cm.gray, aspect=1)\n",
        "  axs[0,m].set_xticks([])\n",
        "  axs[0,m].set_yticks([])\n",
        "  k += 1\n",
        "\n",
        "k = 0\n",
        "for m in range(10):\n",
        "  axs[1,m].imshow(Y_train_batch[k,size//2,:,:,0], interpolation='spline16', aspect=1)\n",
        "  axs[1,m].set_xticks([])\n",
        "  axs[1,m].set_yticks([])\n",
        "  k += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKc55nTzHlkY"
      },
      "source": [
        "And train our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFXM6EJOwVFR"
      },
      "source": [
        "pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_xamzB3toAR"
      },
      "source": [
        "# Load the TensorBoard notebook extension\r\n",
        "%load_ext tensorboard\r\n",
        "\r\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYjmq_7aGUYP"
      },
      "source": [
        "# Define model\n",
        "def down_block(x, filters, kernel_size=(3, 3, 3), padding=\"same\", strides=1):\n",
        "    c = tf.keras.layers.Conv3D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
        "    c = tf.keras.layers.Conv3D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
        "    p = tf.keras.layers.MaxPool3D((2, 2, 2), (2, 2, 2))(c)\n",
        "    return c, p\n",
        "\n",
        "def up_block(x, skip, filters, kernel_size=(3, 3, 3), padding=\"same\", strides=1):\n",
        "    us = tf.keras.layers.UpSampling3D((2, 2, 2))(x)\n",
        "    concat = tf.keras.layers.Concatenate()([us, skip])\n",
        "    c = tf.keras.layers.Conv3D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\n",
        "    c = tf.keras.layers.Conv3D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
        "    return c\n",
        "\n",
        "def bottleneck(x, filters, kernel_size=(3, 3, 3), padding=\"same\", strides=1):\n",
        "    c = tf.keras.layers.Conv3D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
        "    c = tf.keras.layers.Conv3D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
        "    return c\n",
        "\n",
        "class CustomModel(tf.keras.models.Model):\n",
        "    def train_step(self, from_generator):\n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "\n",
        "        x, y, mask = from_generator\n",
        "\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = self(x, training=True)  # Forward pass\n",
        "            # Compute the loss value.\n",
        "            # The loss function is configured in `compile()`.\n",
        "            loss = self.compiled_loss(y, logits, mask)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics.\n",
        "        # Metrics are configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, logits)\n",
        "\n",
        "        # Return a dict mapping metric names to current value.\n",
        "        # Note that it will include the loss (tracked in self.metrics).\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "def UNet():\n",
        "    f = [16, 32, 64, 128, 256]\n",
        "    inputs = tf.keras.layers.Input((size, size, size, 1))\n",
        "  \n",
        "    p0 = inputs\n",
        "    c1, p1 = down_block(p0, f[0]) #128 -> 64\n",
        "    c2, p2 = down_block(p1, f[1]) #64 -> 32\n",
        "    c3, p3 = down_block(p2, f[2]) #32 -> 16\n",
        "    c4, p4 = down_block(p3, f[3]) #16->8\n",
        "  \n",
        "    bn = bottleneck(p4, f[4])\n",
        "  \n",
        "    u1 = up_block(bn, c4, f[3]) #8 -> 16\n",
        "    u2 = up_block(u1, c3, f[2]) #16 -> 32\n",
        "    u3 = up_block(u2, c2, f[1]) #32 -> 64\n",
        "    u4 = up_block(u3, c1, f[0]) #64 -> 128\n",
        "  \n",
        "    outputs = tf.keras.layers.Conv3D(1, (1, 1, 1), padding=\"same\", activation=\"sigmoid\")(u4)\n",
        "    model = CustomModel(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "model = UNet()\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.AUTO)\n",
        "\n",
        "loss2 = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.AUTO)\n",
        "\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def loss3(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    param:\n",
        "    y_pred - Predicted labels\n",
        "    y_true - True labels \n",
        "    Returns:\n",
        "    Specificity score\n",
        "    \"\"\"\n",
        "    neg_y_true = 1 - y_true\n",
        "    neg_y_pred = 1 - y_pred\n",
        "    fp = K.sum(neg_y_true * y_pred)\n",
        "    fn = K.sum(y_true * neg_y_pred)\n",
        "    w0 = K.sum(neg_y_true)\n",
        "    w1 = K.sum(y_true)\n",
        "    \n",
        "    return fp/w1 + fn/w0\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "loss4 = tfa.losses.SigmoidFocalCrossEntropy(alpha=0.9) # extremely skewed\n",
        "\n",
        "\n",
        "metrics = [tf.keras.metrics.BinaryAccuracy(name='acc'),\n",
        "           tf.keras.metrics.MeanIoU(num_classes=2, name='iou'),\n",
        "           tf.keras.metrics.TruePositives(name='TP'),\n",
        "           tf.keras.metrics.FalsePositives(name='FP'),\n",
        "           tf.keras.metrics.TrueNegatives(name='TN'),\n",
        "           tf.keras.metrics.FalseNegatives(name='FN')]\n",
        "           \n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=loss4, metrics=metrics)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95tJCWO3tl-Q"
      },
      "source": [
        "import datetime, os\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(\"./models/trained/model.h5\", verbose=1, save_best_only=True)\n",
        "\n",
        "history =  model.fit(datagen_train,\n",
        "                     steps_per_epoch  = len(datagen_train),\n",
        "                     validation_data  = datagen_val,\n",
        "                     validation_steps = len(datagen_val),\n",
        "                     epochs=100,\n",
        "                     callbacks=[tensorboard, checkpointer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h8O2pUMqU4D"
      },
      "source": [
        "Confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds-aeUfZqYSO"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "\r\n",
        "num_batches = num_val//batch_size\r\n",
        "\r\n",
        "\r\n",
        "cm = np.zeros((2,2))\r\n",
        "\r\n",
        "for n in range(num_batches):\r\n",
        "\r\n",
        "  (X, Y_true, mask) = next(datagen_val)\r\n",
        "  Y_pred = model.predict(X)\r\n",
        "\r\n",
        "  y_true = Y_true[:,:,:,0].round().flatten()\r\n",
        "  y_pred = Y_pred[:,:,:,0].round().flatten()\r\n",
        "\r\n",
        "  cm = cm + confusion_matrix(y_true, y_pred)\r\n",
        "\r\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkFhvJHWn3wX"
      },
      "source": [
        "cmap=plt.cm.Blues\r\n",
        "normalize = True\r\n",
        "\r\n",
        "title = 'Normalized confusion matrix'\r\n",
        "\r\n",
        "\r\n",
        "# Only use the labels that appear in the data\r\n",
        "classes = ['No fault','Fault']\r\n",
        "\r\n",
        "# Normalize\r\n",
        "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n",
        "\r\n",
        "\r\n",
        "fig, ax = plt.subplots()\r\n",
        "im = ax.imshow(cm_norm, interpolation='nearest', cmap=cmap)\r\n",
        "ax.figure.colorbar(im, ax=ax)\r\n",
        "# We want to show all ticks...\r\n",
        "ax.set(xticks=np.arange(cm_norm.shape[1]),\r\n",
        "       yticks=np.arange(cm_norm.shape[0]),\r\n",
        "       # ... and label them with the respective list entries\r\n",
        "       xticklabels=classes, yticklabels=classes,\r\n",
        "       title=title,\r\n",
        "       ylabel='True label',\r\n",
        "       xlabel='Predicted label')\r\n",
        "\r\n",
        "# Rotate the tick labels and set their alignment.\r\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\r\n",
        "         rotation_mode=\"anchor\")\r\n",
        "\r\n",
        "# Loop over data dimensions and create text annotations.\r\n",
        "fmt = '.2f' if normalize else 'd'\r\n",
        "thresh = cm_norm.max() / 2.\r\n",
        "for i in range(cm_norm.shape[0]):\r\n",
        "    for j in range(cm_norm.shape[1]):\r\n",
        "        ax.text(j, i, format(cm_norm[i, j], fmt),\r\n",
        "                ha=\"center\", va='center',\r\n",
        "                color=\"white\" if cm_norm[i, j] > thresh else \"black\")\r\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFqNfE7UqYbw"
      },
      "source": [
        "and look at a couple of examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThjWg1DprsJU"
      },
      "source": [
        "X_val_batch, Y_val_batch, mask_val_batch = next(datagen_val)\n",
        "\n",
        "fig, axs = plt.subplots(3, 10, figsize=(15,5))\n",
        "\n",
        "k = 0\n",
        "for m in range(10):\n",
        "  axs[0,m].imshow(X_val_batch[k,size//2,:,:,0], interpolation='spline16', cmap=plt.cm.gray, aspect=1)\n",
        "  axs[0,m].set_xticks([])\n",
        "  axs[0,m].set_yticks([])\n",
        "  k += 1\n",
        "\n",
        "\n",
        "k = 0\n",
        "for m in range(10):\n",
        "  axs[1,m].imshow(Y_val_batch[k,size//2,:,:,0], interpolation='spline16', aspect=1)\n",
        "  axs[1,m].set_xticks([])\n",
        "  axs[1,m].set_yticks([])\n",
        "  k += 1\n",
        "\n",
        "\n",
        "\n",
        "Y_predict_batch = model.predict_on_batch(X_val_batch)\n",
        "k = 0\n",
        "for m in range(10):\n",
        "  axs[2,m].imshow(Y_predict_batch[k,size//2,:,:,0], interpolation='spline16', aspect=1)\n",
        "  axs[2,m].set_xticks([])\n",
        "  axs[2,m].set_yticks([])\n",
        "  k += 1\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRSG7MIrslAw"
      },
      "source": [
        "def tile_3D(data, wsize, dt, dx):    \n",
        "    (_,t_max, x_max) = data.shape     \n",
        "    n_patch = (t_max//dt+1)*(x_max//dx+1)\n",
        "    data_patch = np.zeros((n_patch,wsize,wsize,wsize,1)) \n",
        "    n = 0\n",
        "    for t in range(0, t_max, dt):\n",
        "        for x in range(0, x_max, dx):    \n",
        "            if t_max - t < wsize and x_max - x < wsize:\n",
        "                data_patch[n,:,:,:,0] = data[:,t_max-wsize:t_max, x_max-wsize:x_max]\n",
        "            elif x_max - x < wsize:\n",
        "                data_patch[n,:,:,:,0] = data[:,t:t+wsize, x_max-wsize:x_max]\n",
        "            elif t_max - t < wsize:\n",
        "                data_patch[n,:,:,:,0] = data[:,t_max-wsize:t_max, x:x+wsize]\n",
        "            else:\n",
        "                data_patch[n,:,:,:,0] = data[:,t:t+wsize,x:x+wsize]  \n",
        "            n = n + 1    \n",
        "    return data_patch \n",
        "\n",
        "\n",
        "\n",
        "def merge_3D(data_patch, t_max, x_max, wsize, dt, dx):\n",
        "    data_new = np.zeros((wsize, t_max, x_max))\n",
        "    count    = np.zeros((wsize, t_max, x_max))    \n",
        "    n = 0\n",
        "    for t in range(0, t_max, dt):\n",
        "        for x in range(0, x_max, dx):\n",
        "            if t_max - t < wsize and x_max - x < wsize:\n",
        "                data_new[:,t_max-wsize:t_max, x_max-wsize:x_max] += data_patch[n,:,:,:,0]\n",
        "                count[:,t_max-wsize:t_max, x_max-wsize:x_max] += 1\n",
        "            elif x_max - x < wsize:\n",
        "                data_new[:,t:t+wsize, x_max-wsize:x_max] += data_patch[n,:,:,:,0]\n",
        "                count[:,t:t+wsize, x_max-wsize:x_max] += 1 \n",
        "            elif t_max - t < wsize:\n",
        "                data_new[:,t_max-wsize:t_max, x:x+wsize] += data_patch[n,:,:,:,0]\n",
        "                count[:,t_max-wsize:t_max, x:x+wsize] += 1\n",
        "            else:\n",
        "                data_new[:,t:t+wsize,x:x+wsize] += data_patch[n,:,:,:,0]\n",
        "                count[:,t:t+wsize,x:x+wsize] += 1 \n",
        "            n = n + 1\n",
        "    return data_new/count\n",
        "\n",
        "\n",
        "dt = 16\n",
        "dx = 16\n",
        "\n",
        "prediction = np.zeros_like(data)\n",
        "\n",
        "times = [0,100,200,300,400,462]\n",
        "\n",
        "for m in range(size//2, 651-size//2, size):\n",
        "\n",
        "  print(str(round(m/651*100)) + '%')\n",
        "  \n",
        "  for n in range(5):\n",
        "      cube = data[m-size//2:m+size//2,times[n]:times[n+1],:]\n",
        "      cube_tiles = tile_3D(cube, size, dt, dx)\n",
        "      result_tiles = model.predict_on_batch(cube_tiles)\n",
        "      (_, t_max, x_max) = cube.shape\n",
        "      result       = merge_3D(result_tiles, t_max, x_max, size, dt, dx)\n",
        "      prediction[100-size//2:100+size//2,times[n]:times[n+1]] = result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKMt9WlHa85d"
      },
      "source": [
        "Now let's look at our predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJXhwe-NUW26"
      },
      "source": [
        "    plt.figure(figsize=(10,10))    \r\n",
        "    plot_overlay(data[100,:,:], prediction[100,:,:])\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
